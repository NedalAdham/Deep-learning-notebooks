{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:48:51.137474Z",
     "start_time": "2025-05-09T21:48:50.640720Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier  # Import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T21:58:59.031262Z",
     "start_time": "2025-05-09T21:58:59.024357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data loading\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load a CSV file and return a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the DataFrame by selecting relevant features and handling missing values.\"\"\"\n",
    "    # Select relevant feature columns\n",
    "    feature_cols = [\n",
    "        'acc_X', 'acc_Y', 'acc_Z',\n",
    "        'mag_X', 'mag_Y', 'mag_Z',\n",
    "        'gyro_X', 'gyro_Y', 'gyro_Z'\n",
    "    ]\n",
    "    selected_data = df[feature_cols]\n",
    "\n",
    "    # Handle missing values (fill with zeros)\n",
    "    selected_data = selected_data.fillna(0)\n",
    "\n",
    "    # Extract the target label column\n",
    "    labels = df['activity']\n",
    "\n",
    "    return selected_data, labels\n"
   ],
   "id": "c91b2f8cb9a7cb81",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T21:59:02.140763Z",
     "start_time": "2025-05-09T21:59:02.135010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Normalization\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalize the data using MinMaxScaler.\"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "    return normalized_data\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(window):\n",
    "    \"\"\"Extract statistical features from a window of data.\"\"\"\n",
    "    features = {}\n",
    "    for col in window.columns:\n",
    "        if len(window[col].unique()) > 1:  # Avoid constant columns\n",
    "            features[f'{col}_mean'] = window[col].mean()\n",
    "            features[f'{col}_std'] = window[col].std()\n",
    "            features[f'{col}_min'] = window[col].min()\n",
    "            features[f'{col}_max'] = window[col].max()\n",
    "        else:\n",
    "            # For constant columns\n",
    "            features[f'{col}_mean'] = window[col].mean()\n",
    "            features[f'{col}_std'] = 0\n",
    "            features[f'{col}_min'] = window[col].min()\n",
    "            features[f'{col}_max'] = window[col].max()\n",
    "    return features"
   ],
   "id": "a2c22585463bbd3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T21:59:06.404259Z",
     "start_time": "2025-05-09T21:59:06.399241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_feature_dataset(data, labels, window_size=50, step_size=25):\n",
    "    \"\"\"Create a dataset of features and corresponding labels from sliding windows.\"\"\"\n",
    "    X, y = [], []\n",
    "    for start in range(0, len(data) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = data.iloc[start:end]\n",
    "        label_window = labels.iloc[start:end]\n",
    "\n",
    "        # Extract features from the window\n",
    "        features = extract_features(window)\n",
    "        X.append(features)\n",
    "\n",
    "        # Assign the most frequent label in the window as the target label\n",
    "        label = label_window.mode().iloc[0]  # Most frequent label\n",
    "        y.append(label)\n",
    "\n",
    "    # Convert to DataFrame and Series\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    return X, y"
   ],
   "id": "bec48c3f34d6ecd4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T22:07:30.708348Z",
     "start_time": "2025-05-09T22:07:30.699732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "def train_adaboost_model(X, y, save_path='movement_detection_adaboost_model.pkl', label_encoder_path='adaboost_label_encoder.pkl'):\n",
    "    \"\"\"Train and save an AdaBoost model.\"\"\"\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train AdaBoost classifier\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"\\n--- AdaBoost Classification Report ---\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    # Save model and encoder\n",
    "    joblib.dump(clf, save_path)\n",
    "    joblib.dump(label_encoder, label_encoder_path)\n",
    "    print(f\"AdaBoost model saved as '{save_path}'\")\n",
    "    print(f\"Label encoder saved as '{label_encoder_path}'\")\n",
    "\n",
    "    return clf, label_encoder\n"
   ],
   "id": "1cf6bb9580d4c364",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T22:07:33.499728Z",
     "start_time": "2025-05-09T22:07:33.491315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Balancing Data using SMOTE\n",
    "def balance_data(X, y):\n",
    "    \"\"\"Balance the dataset using SMOTE.\"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    return X_balanced, y_balanced"
   ],
   "id": "34caf251695f3d78",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T22:07:34.712303Z",
     "start_time": "2025-05-09T22:07:34.705355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "def save_model(model, label_encoder, output_path):\n",
    "    \"\"\"Save the trained model and label encoder to files.\"\"\"\n",
    "    joblib.dump(model, output_path)\n",
    "    joblib.dump(label_encoder, output_path.replace('.pkl', '_label_encoder.pkl'))\n",
    "    print(f\"Model saved as '{output_path}'\")\n",
    "    print(f\"Label encoder saved as '{output_path.replace('.pkl', '_label_encoder.pkl')}'\")\n"
   ],
   "id": "5e173604ca328051",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T22:19:47.068204Z",
     "start_time": "2025-05-09T22:19:47.059878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main workflow\n",
    "def main(file_path):\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Display unique activity labels\n",
    "    unique_activities = df['activity'].unique()\n",
    "    print(\"Unique activity labels:\", unique_activities)\n",
    "\n",
    "    data, labels = preprocess_data(df)\n",
    "    print(\"Data preprocessed successfully.\")\n",
    "\n",
    "    # Normalize data\n",
    "    print(\"Normalizing data...\")\n",
    "    data_normalized = normalize_data(data)\n",
    "    print(\"Data normalized successfully.\")\n",
    "\n",
    "    # Balance data\n",
    "    print(\"Balancing data using SMOTE...\")\n",
    "    X_balanced, y_balanced = balance_data(data_normalized, labels)\n",
    "    print(\"Data balanced successfully.\")\n",
    "\n",
    "    # Extract features and labels\n",
    "    print(\"Extracting features...\")\n",
    "    X, y = create_feature_dataset(X_balanced, y_balanced)\n",
    "    print(\"Feature extraction completed.\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model_adaboost, label_encoder = train_adaboost_model(X, y)\n",
    "\n",
    "\n",
    "\n",
    "    # Save the model\n",
    "    save_model(model_adaboost, label_encoder, 'movement_detection_adaboost_model.pkl')\n",
    "\n",
    "    return model_adaboost, label_encoder\n",
    "\n"
   ],
   "id": "dac33cb14ad08735",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T22:12:15.336459Z",
     "start_time": "2025-05-09T22:11:00.041371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the main workflow\n",
    "main('.idea/df.csv')"
   ],
   "id": "65f36940ea776a84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "Unique activity labels: ['downstairs' 'running' 'standing' 'upstairs' 'walking']\n",
      "Data preprocessed successfully.\n",
      "Normalizing data...\n",
      "Data normalized successfully.\n",
      "Balancing data using SMOTE...\n",
      "Data balanced successfully.\n",
      "Extracting features...\n",
      "Feature extraction completed.\n",
      "Training the model...\n",
      "\n",
      "--- AdaBoost Classification Report ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.48      0.39      0.43       680\n",
      "     running       0.87      0.79      0.83       631\n",
      "    standing       0.83      0.79      0.81       702\n",
      "    upstairs       0.55      0.62      0.58       712\n",
      "     walking       0.58      0.69      0.63       705\n",
      "\n",
      "    accuracy                           0.65      3430\n",
      "   macro avg       0.66      0.66      0.66      3430\n",
      "weighted avg       0.66      0.65      0.65      3430\n",
      "\n",
      "AdaBoost model saved as 'movement_detection_adaboost_model.pkl'\n",
      "Label encoder saved as 'adaboost_label_encoder.pkl'\n",
      "Model saved as 'movement_detection_adaboost_model.pkl'\n",
      "Label encoder saved as 'movement_detection_adaboost_model_label_encoder.pkl'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdaBoostClassifier(n_estimators=100, random_state=42), LabelEncoder())"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb13d4766d85d2ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "333373cadb78d31c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T11:24:58.175084Z",
     "start_time": "2025-05-11T11:23:59.682704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load dataset from CSV\n",
    "def read_csv_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# Select and clean data\n",
    "def select_features_and_labels(dataframe):\n",
    "    sensors = ['acc_X', 'acc_Y', 'acc_Z', 'mag_X', 'mag_Y', 'mag_Z', 'gyro_X', 'gyro_Y', 'gyro_Z']\n",
    "    features = dataframe[sensors].fillna(0)\n",
    "    targets = dataframe['activity']\n",
    "    return features, targets\n",
    "\n",
    "# Normalize the sensor data\n",
    "def scale_features(features):\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "# Create statistical features per time window\n",
    "def get_window_features(window):\n",
    "    stats = {}\n",
    "    for col in window.columns:\n",
    "        values = window[col]\n",
    "        stats[f'{col}_mean'] = values.mean()\n",
    "        stats[f'{col}_std'] = values.std() if values.std() != np.nan else 0\n",
    "        stats[f'{col}_min'] = values.min()\n",
    "        stats[f'{col}_max'] = values.max()\n",
    "    return stats\n",
    "\n",
    "# Segment data and aggregate features\n",
    "def generate_sliding_windows(data, labels, window_size=50, step=25):\n",
    "    features, targets = [], []\n",
    "    for i in range(0, len(data) - window_size, step):\n",
    "        segment = data.iloc[i:i+window_size]\n",
    "        segment_label = labels.iloc[i:i+window_size].mode().iloc[0]\n",
    "        features.append(get_window_features(segment))\n",
    "        targets.append(segment_label)\n",
    "    return pd.DataFrame(features), pd.Series(targets)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "def resample_data(X, y):\n",
    "    return SMOTE(random_state=42).fit_resample(X, y)\n",
    "\n",
    "# Train the AdaBoost classifier and save components\n",
    "def fit_and_store_model(X, y, model_file='movement_detection_adaboost_model.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"\\n--- AdaBoost Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_test, predictions, target_names=le.classes_))\n",
    "\n",
    "    joblib.dump(model, model_file)\n",
    "    joblib.dump(le, model_file.replace('.pkl', '_label_encoder.pkl'))\n",
    "    print(f\"Saved model to {model_file} and label encoder.\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# Main control function\n",
    "def execute_pipeline(csv_path):\n",
    "    print(\"Starting pipeline...\")\n",
    "    df = read_csv_data(csv_path)\n",
    "    X_raw, y_raw = select_features_and_labels(df)\n",
    "    X_scaled = scale_features(X_raw)\n",
    "    print(\"Preprocessing complete.\")\n",
    "\n",
    "    X_balanced, y_balanced = resample_data(X_scaled, y_raw)\n",
    "    print(\"Balancing done.\")\n",
    "\n",
    "    X_final, y_final = generate_sliding_windows(X_balanced, y_balanced)\n",
    "    print(\"Feature extraction complete.\")\n",
    "\n",
    "    model, encoder = fit_and_store_model(X_final, y_final)\n",
    "    print(\"Training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "# Trigger the workflow\n",
    "execute_pipeline('.idea/df.csv')\n"
   ],
   "id": "746e326dae4fdbe6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline...\n",
      "Preprocessing complete.\n",
      "Balancing done.\n",
      "Feature extraction complete.\n",
      "\n",
      "--- AdaBoost Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.48      0.39      0.43       680\n",
      "     running       0.87      0.79      0.83       631\n",
      "    standing       0.83      0.79      0.81       702\n",
      "    upstairs       0.55      0.62      0.58       712\n",
      "     walking       0.58      0.69      0.63       705\n",
      "\n",
      "    accuracy                           0.65      3430\n",
      "   macro avg       0.66      0.66      0.66      3430\n",
      "weighted avg       0.66      0.65      0.65      3430\n",
      "\n",
      "Saved model to movement_detection_adaboost_model.pkl and label encoder.\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdaBoostClassifier(n_estimators=100, random_state=42), LabelEncoder())"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:04:23.215756Z",
     "start_time": "2025-05-13T00:02:48.429628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. Load CSV Data\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# 2. Select and clean features\n",
    "def select_features(df):\n",
    "    features = ['acc_X', 'acc_Y', 'acc_Z', 'mag_X', 'mag_Y', 'mag_Z', 'gyro_X', 'gyro_Y', 'gyro_Z']\n",
    "    X = df[features].fillna(0)\n",
    "    y = df['activity']\n",
    "    return X, y\n",
    "\n",
    "# 3. Normalize data\n",
    "def scale_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 4. Windowed feature extraction\n",
    "def extract_stat_features(window):\n",
    "    stats = {}\n",
    "    for col in window.columns:\n",
    "        stats[f'{col}_mean'] = window[col].mean()\n",
    "        stats[f'{col}_std'] = window[col].std() if window[col].nunique() > 1 else 0\n",
    "        stats[f'{col}_min'] = window[col].min()\n",
    "        stats[f'{col}_max'] = window[col].max()\n",
    "    return stats\n",
    "\n",
    "# 5. Sliding window feature creation\n",
    "def generate_sliding_windows(X, y, size=50, step=25):\n",
    "    X_feat, y_labels = [], []\n",
    "    for start in range(0, len(X) - size, step):\n",
    "        window = X.iloc[start:start + size]\n",
    "        label_window = y.iloc[start:start + size]\n",
    "        stats = extract_stat_features(window)\n",
    "        X_feat.append(stats)\n",
    "        y_labels.append(label_window.mode()[0])\n",
    "    return pd.DataFrame(X_feat), pd.Series(y_labels)\n",
    "\n",
    "# 6. Balance the dataset\n",
    "def rebalance(X, y):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    return sm.fit_resample(X, y)\n",
    "\n",
    "# 7. Train AdaBoost and save\n",
    "def fit_and_store_model(X, y, model_file='adaboost_model.pkl', encoder_file='label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
    "                               n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- AdaBoost Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_test, predictions, target_names=le.classes_))\n",
    "\n",
    "    joblib.dump(model, model_file)\n",
    "    joblib.dump(le, encoder_file)\n",
    "    print(f\"Model saved to: {model_file}\")\n",
    "    print(f\"Label encoder saved to: {encoder_file}\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# 8. Main runner\n",
    "def execute_pipeline(csv_path):\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_csv(csv_path)\n",
    "    X_raw, y_raw = select_features(df)\n",
    "    X_scaled = scale_data(X_raw)\n",
    "\n",
    "    print(\"Balancing dataset using SMOTE...\")\n",
    "    X_balanced, y_balanced = rebalance(X_scaled, y_raw)\n",
    "\n",
    "    print(\"Extracting windowed features...\")\n",
    "    X_final, y_final = generate_sliding_windows(X_balanced, y_balanced)\n",
    "    print(\"Feature extraction complete.\")\n",
    "\n",
    "    model, encoder = fit_and_store_model(X_final, y_final)\n",
    "    print(\"Training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "# Run the pipeline\n",
    "execute_pipeline('.idea/df.csv')\n"
   ],
   "id": "744138d32c94e920",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Balancing dataset using SMOTE...\n",
      "Extracting windowed features...\n",
      "Feature extraction complete.\n",
      "\n",
      "--- AdaBoost Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.98      0.95      0.97       680\n",
      "     running       0.99      0.98      0.98       631\n",
      "    standing       0.99      0.99      0.99       702\n",
      "    upstairs       0.94      0.97      0.95       712\n",
      "     walking       0.95      0.95      0.95       705\n",
      "\n",
      "    accuracy                           0.97      3430\n",
      "   macro avg       0.97      0.97      0.97      3430\n",
      "weighted avg       0.97      0.97      0.97      3430\n",
      "\n",
      "Model saved to: adaboost_model.pkl\n",
      "Label encoder saved to: label_encoder.pkl\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                    n_estimators=100, random_state=42),\n",
       " LabelEncoder())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:41:43.959780Z",
     "start_time": "2025-05-14T15:31:22.558604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. Load CSV Data\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# 2. Select and clean features\n",
    "def select_features(df):\n",
    "    features = ['acc_X', 'acc_Y', 'acc_Z', 'mag_X', 'mag_Y', 'mag_Z', 'gyro_X', 'gyro_Y', 'gyro_Z']\n",
    "    X = df[features].fillna(0)\n",
    "    y = df['activity']\n",
    "    return X, y\n",
    "\n",
    "# 3. Normalize data\n",
    "def scale_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 4. Extract statistical features for a single window\n",
    "def extract_stat_features(window):\n",
    "    stats = {}\n",
    "    for col in window.columns:\n",
    "        stats[f'{col}_mean'] = window[col].mean()\n",
    "        stats[f'{col}_std'] = window[col].std()\n",
    "        stats[f'{col}_min'] = window[col].min()\n",
    "        stats[f'{col}_max'] = window[col].max()\n",
    "    return stats\n",
    "\n",
    "# 5. Fixed-size sliding window (in samples)\n",
    "def generate_windows_by_sample_count(X, y, size=10, step=5):\n",
    "    X_feat, y_labels = [], []\n",
    "    for start in range(0, len(X) - size, step):\n",
    "        window = X.iloc[start:start + size]\n",
    "        label_window = y.iloc[start:start + size]\n",
    "        stats = extract_stat_features(window)\n",
    "        X_feat.append(stats)\n",
    "        y_labels.append(label_window.mode()[0])  # Assign most common label in window\n",
    "    return pd.DataFrame(X_feat), pd.Series(y_labels)\n",
    "\n",
    "# 6. Balance dataset using SMOTE\n",
    "def rebalance(X, y):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    return sm.fit_resample(X, y)\n",
    "\n",
    "# 7. Train and save AdaBoost model\n",
    "def fit_and_store_model(X, y, model_file='adaboost_model.pkl', encoder_file='label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=5),\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- AdaBoost Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_test, predictions, target_names=le.classes_))\n",
    "\n",
    "    joblib.dump(model, model_file)\n",
    "    joblib.dump(le, encoder_file)\n",
    "    print(f\"‚úÖ Model saved to: {model_file}\")\n",
    "    print(f\"‚úÖ Label encoder saved to: {encoder_file}\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# 8. Main pipeline\n",
    "def execute_pipeline(csv_path, window_size=10, window_step=5):\n",
    "    print(\"üì• Loading and preparing data...\")\n",
    "    df = load_csv(csv_path)\n",
    "    X_raw, y_raw = select_features(df)\n",
    "    X_scaled = scale_data(X_raw)\n",
    "\n",
    "    print(\"üîÅ Balancing dataset using SMOTE...\")\n",
    "    X_balanced, y_balanced = rebalance(X_scaled, y_raw)\n",
    "\n",
    "    print(f\"ü™ü Creating sliding windows of {window_size} samples with step {window_step}...\")\n",
    "    X_final, y_final = generate_windows_by_sample_count(X_balanced, y_balanced, size=window_size, step=window_step)\n",
    "    print(\"üìä Feature extraction complete.\")\n",
    "\n",
    "    model, encoder = fit_and_store_model(X_final, y_final)\n",
    "    print(\"‚úÖ Training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "# üöÄ Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline(\".idea/df.csv\", window_size=50, window_step=5)\n"
   ],
   "id": "23dbcbea6d2bf5f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading and preparing data...\n",
      "üîÅ Balancing dataset using SMOTE...\n",
      "ü™ü Creating sliding windows of 50 samples with step 5...\n",
      "üìä Feature extraction complete.\n",
      "\n",
      "--- AdaBoost Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.99      0.96      0.98      3413\n",
      "     running       1.00      0.99      0.99      3393\n",
      "    standing       1.00      1.00      1.00      3531\n",
      "    upstairs       0.96      0.98      0.97      3346\n",
      "     walking       0.98      0.98      0.98      3467\n",
      "\n",
      "    accuracy                           0.98     17150\n",
      "   macro avg       0.98      0.98      0.98     17150\n",
      "weighted avg       0.98      0.98      0.98     17150\n",
      "\n",
      "‚úÖ Model saved to: adaboost_model.pkl\n",
      "‚úÖ Label encoder saved to: label_encoder.pkl\n",
      "‚úÖ Training complete.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:50:24.810738Z",
     "start_time": "2025-05-14T15:41:45.670185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. Load CSV Data\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# 2. Select and clean features\n",
    "def select_features(df):\n",
    "    features = ['acc_X', 'acc_Y', 'acc_Z', 'mag_X', 'mag_Y', 'mag_Z', 'gyro_X', 'gyro_Y', 'gyro_Z']\n",
    "    X = df[features].fillna(0)\n",
    "    y = df['activity']\n",
    "    return X, y\n",
    "\n",
    "# 3. Normalize data\n",
    "def scale_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 4. Extract statistical features for a single window\n",
    "def extract_stat_features(window):\n",
    "    stats = {}\n",
    "    for col in window.columns:\n",
    "        stats[f'{col}_mean'] = window[col].mean()\n",
    "        stats[f'{col}_std'] = window[col].std()\n",
    "        stats[f'{col}_min'] = window[col].min()\n",
    "        stats[f'{col}_max'] = window[col].max()\n",
    "    return stats\n",
    "\n",
    "# 5. Fixed-size sliding window (in samples)\n",
    "def generate_windows_by_sample_count(X, y, size=10, step=5):\n",
    "    X_feat, y_labels = [], []\n",
    "    for start in range(0, len(X) - size, step):\n",
    "        window = X.iloc[start:start + size]\n",
    "        label_window = y.iloc[start:start + size]\n",
    "        stats = extract_stat_features(window)\n",
    "        X_feat.append(stats)\n",
    "        y_labels.append(label_window.mode()[0])  # Assign most common label in window\n",
    "    return pd.DataFrame(X_feat), pd.Series(y_labels)\n",
    "\n",
    "# 6. Train and save AdaBoost model\n",
    "def fit_and_store_model(X, y, model_file='adaboost_model.pkl', encoder_file='label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=5),\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n--- AdaBoost Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_test, predictions, target_names=le.classes_))\n",
    "\n",
    "    joblib.dump(model, model_file)\n",
    "    joblib.dump(le, encoder_file)\n",
    "    print(f\"‚úÖ Model saved to: {model_file}\")\n",
    "    print(f\"‚úÖ Label encoder saved to: {encoder_file}\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# 7. Main pipeline\n",
    "def execute_pipeline(csv_path, window_size=10, window_step=5):\n",
    "    print(\"üì• Loading and preparing data...\")\n",
    "    df = load_csv(csv_path)\n",
    "    X_raw, y_raw = select_features(df)\n",
    "    X_scaled = scale_data(X_raw)\n",
    "\n",
    "    print(f\"ü™ü Creating sliding windows of {window_size} samples with step {window_step}...\")\n",
    "    X_final, y_final = generate_windows_by_sample_count(X_scaled, y_raw, size=window_size, step=window_step)\n",
    "    print(\"üìä Feature extraction complete.\")\n",
    "\n",
    "    model, encoder = fit_and_store_model(X_final, y_final)\n",
    "    print(\"‚úÖ Training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "# üöÄ Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    execute_pipeline(\".idea/df.csv\", window_size=50, window_step=5)\n"
   ],
   "id": "ff06f8aeb671df2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading and preparing data...\n",
      "ü™ü Creating sliding windows of 50 samples with step 5...\n",
      "üìä Feature extraction complete.\n",
      "\n",
      "--- AdaBoost Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.98      0.97      0.98      3282\n",
      "     running       1.00      0.99      1.00      1918\n",
      "    standing       1.00      1.00      1.00      3275\n",
      "    upstairs       0.94      0.98      0.96      2885\n",
      "     walking       0.98      0.96      0.97      3483\n",
      "\n",
      "    accuracy                           0.98     14843\n",
      "   macro avg       0.98      0.98      0.98     14843\n",
      "weighted avg       0.98      0.98      0.98     14843\n",
      "\n",
      "‚úÖ Model saved to: adaboost_model.pkl\n",
      "‚úÖ Label encoder saved to: label_encoder.pkl\n",
      "‚úÖ Training complete.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "\n",
    "# 1. Load Data\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# 2. Feature Selection\n",
    "def select_features(df):\n",
    "    features = ['acc_X', 'acc_Y', 'acc_Z', 'mag_X', 'mag_Y', 'mag_Z', 'gyro_X', 'gyro_Y', 'gyro_Z']\n",
    "    return df[features].fillna(0), df['activity']\n",
    "\n",
    "# 3. Normalize\n",
    "def scale_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 4. Windowed Data Generator for LSTM\n",
    "def create_lstm_windows(X, y, window_size=50, step=25):\n",
    "    X_seq, y_seq = [], []\n",
    "    for start in range(0, len(X) - window_size, step):\n",
    "        end = start + window_size\n",
    "        segment = X.iloc[start:end].values\n",
    "        label_window = y.iloc[start:end]\n",
    "        label = label_window.mode()[0]\n",
    "        X_seq.append(segment)\n",
    "        y_seq.append(label)\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 5. Apply SMOTE to 2D features before reshaping for LSTM\n",
    "def rebalance(X, y):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_2d = X.reshape((X.shape[0], -1))  # Flatten for SMOTE\n",
    "    X_bal, y_bal = sm.fit_resample(X_2d, y)\n",
    "    X_bal = X_bal.reshape((-1, 50, 9))  # Reshape back\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# 6. Build and Train LSTM Model\n",
    "def build_and_train_lstm(X, y, model_path='lstm_model.h5', encoder_path='lstm_label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(y_categorical.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    print(\"\\n--- LSTM Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "    # Save model and encoder\n",
    "    model.save(model_path)\n",
    "    joblib.dump(le, encoder_path)\n",
    "    print(f\"LSTM model saved to '{model_path}'\")\n",
    "    print(f\"Label encoder saved to '{encoder_path}'\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# 7. Full Runner\n",
    "def run_lstm_pipeline(path):\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_csv(path)\n",
    "    X_raw, y_raw = select_features(df)\n",
    "    X_scaled = scale_data(X_raw)\n",
    "\n",
    "    print(\"Creating LSTM windows...\")\n",
    "    X_windowed, y_windowed = create_lstm_windows(X_scaled, y_raw)\n",
    "\n",
    "    print(\"Balancing data with SMOTE...\")\n",
    "    X_balanced, y_balanced = rebalance(X_windowed, y_windowed)\n",
    "\n",
    "    print(\"Training LSTM model...\")\n",
    "    model, encoder = build_and_train_lstm(X_balanced, y_balanced)\n",
    "    print(\"LSTM training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "# üî• Execute the pipeline\n",
    "run_lstm_pipeline('.idea/df.csv')\n"
   ],
   "id": "73bdc296865a01fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T11:47:48.144494Z",
     "start_time": "2025-05-11T11:45:11.962788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- Step 1: Load and preprocess data ---\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Separate features and label\n",
    "    X = df.drop('activity', axis=1)\n",
    "    y = df['activity']\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "# --- Step 2: Apply SMOTE to balance classes ---\n",
    "def rebalance(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# --- Step 3: Generate sliding windows and statistical features ---\n",
    "def generate_sliding_windows(X, y, window_size=50, step_size=25):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "\n",
    "    for start in range(0, len(X) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = X[start:end]\n",
    "        label = y[start:end]\n",
    "\n",
    "        # Use the most frequent label in the window\n",
    "        unique, counts = np.unique(label, return_counts=True)\n",
    "        dominant_label = unique[np.argmax(counts)]\n",
    "\n",
    "        # Reshape to (window_size, num_features)\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(dominant_label)\n",
    "\n",
    "    return np.array(X_windows), np.array(y_windows)\n",
    "\n",
    "# --- Step 4: Build and train LSTM model ---\n",
    "def build_and_train_lstm(X, y, model_path='lstm_model.h5', encoder_path='lstm_label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=(X.shape[1], X.shape[2])),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(y_categorical.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=25,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(\"\\n--- Improved LSTM Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "    model.save(model_path)\n",
    "    joblib.dump(le, encoder_path)\n",
    "    print(f\"LSTM model saved to '{model_path}'\")\n",
    "    print(f\"Label encoder saved to '{encoder_path}'\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# --- Step 5: Run the full pipeline ---\n",
    "def run_lstm_pipeline(path):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_raw, y_raw = load_and_preprocess_data(path)\n",
    "\n",
    "    print(\"Rebalancing classes using SMOTE...\")\n",
    "    X_balanced, y_balanced = rebalance(X_raw, y_raw)\n",
    "\n",
    "    print(\"Generating sliding windows and extracting features...\")\n",
    "    X_windowed, y_windowed = generate_sliding_windows(X_balanced, y_balanced)\n",
    "\n",
    "    print(\"Training LSTM model...\")\n",
    "    model, encoder = build_and_train_lstm(X_windowed, y_windowed)\n",
    "\n",
    "    print(\"LSTM training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "# üî• Execute the pipeline\n",
    "run_lstm_pipeline('.idea/df.csv')\n"
   ],
   "id": "b5c43541c46ba980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Rebalancing classes using SMOTE...\n",
      "Generating sliding windows and extracting features...\n",
      "Training LSTM model...\n",
      "Epoch 1/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 93ms/step - accuracy: 0.5215 - loss: 1.1679 - val_accuracy: 0.7598 - val_loss: 0.6901\n",
      "Epoch 2/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 89ms/step - accuracy: 0.7697 - loss: 0.6480 - val_accuracy: 0.7945 - val_loss: 0.5959\n",
      "Epoch 3/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 104ms/step - accuracy: 0.8046 - loss: 0.5664 - val_accuracy: 0.8243 - val_loss: 0.5128\n",
      "Epoch 4/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 95ms/step - accuracy: 0.8349 - loss: 0.4760 - val_accuracy: 0.8513 - val_loss: 0.4058\n",
      "Epoch 5/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 89ms/step - accuracy: 0.8593 - loss: 0.4079 - val_accuracy: 0.8724 - val_loss: 0.3698\n",
      "Epoch 6/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 84ms/step - accuracy: 0.8892 - loss: 0.3390 - val_accuracy: 0.8950 - val_loss: 0.3150\n",
      "Epoch 7/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 92ms/step - accuracy: 0.8981 - loss: 0.3012 - val_accuracy: 0.8885 - val_loss: 0.3296\n",
      "Epoch 8/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 88ms/step - accuracy: 0.9069 - loss: 0.2816 - val_accuracy: 0.8863 - val_loss: 0.3424\n",
      "Epoch 9/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 93ms/step - accuracy: 0.9178 - loss: 0.2453 - val_accuracy: 0.8925 - val_loss: 0.3373\n",
      "\u001B[1m108/108\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Improved LSTM Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.88      0.89      0.88       680\n",
      "     running       0.95      0.90      0.93       631\n",
      "    standing       0.93      0.97      0.95       702\n",
      "    upstairs       0.88      0.88      0.88       712\n",
      "     walking       0.85      0.84      0.84       705\n",
      "\n",
      "    accuracy                           0.90      3430\n",
      "   macro avg       0.90      0.90      0.90      3430\n",
      "weighted avg       0.90      0.90      0.90      3430\n",
      "\n",
      "LSTM model saved to 'lstm_model.h5'\n",
      "Label encoder saved to 'lstm_label_encoder.pkl'\n",
      "LSTM training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Sequential name=sequential_1, built=True>, LabelEncoder())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:11:52.703470Z",
     "start_time": "2025-05-11T12:09:00.056939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# --- Step 1: Load and preprocess data ---\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Separate features and label\n",
    "    X = df.drop('activity', axis=1)\n",
    "    y = df['activity']\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "\n",
    "# --- Step 2: Apply SMOTE to balance classes ---\n",
    "def rebalance(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "# --- Step 3: Generate sliding windows and statistical features ---\n",
    "def generate_sliding_windows(X, y, window_size=50, step_size=25):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "\n",
    "    for start in range(0, len(X) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = X[start:end]\n",
    "        label = y[start:end]\n",
    "\n",
    "        # Use the most frequent label in the window\n",
    "        unique, counts = np.unique(label, return_counts=True)\n",
    "        dominant_label = unique[np.argmax(counts)]\n",
    "\n",
    "        # Reshape to (window_size, num_features)\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(dominant_label)\n",
    "\n",
    "    return np.array(X_windows), np.array(y_windows)\n",
    "\n",
    "\n",
    "# --- Step 4: Build and train Transformer model ---\n",
    "def build_and_train_transformer(X, y, model_path='transformer_model.h5', encoder_path='transformer_label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define Transformer model\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "\n",
    "    # Multi-head self-attention\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(y_categorical.shape[1], activation='softmax')(x)\n",
    "\n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=25,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(\"\\n--- Improved Transformer Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "    model.save(model_path)\n",
    "    joblib.dump(le, encoder_path)\n",
    "    print(f\"Transformer model saved to '{model_path}'\")\n",
    "    print(f\"Label encoder saved to '{encoder_path}'\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "\n",
    "# --- Step 5: Convert to TFLite ---\n",
    "def convert_to_tflite(model, tflite_model_path='transformer_model.tflite'):\n",
    "    # Convert the model to TFLite format\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the TFLite model\n",
    "    with open(tflite_model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"TFLite model saved to '{tflite_model_path}'\")\n",
    "\n",
    "\n",
    "# --- Step 6: Run the full pipeline ---\n",
    "def run_transformer_pipeline(path):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_raw, y_raw = load_and_preprocess_data(path)\n",
    "\n",
    "    print(\"Rebalancing classes using SMOTE...\")\n",
    "    X_balanced, y_balanced = rebalance(X_raw, y_raw)\n",
    "\n",
    "    print(\"Generating sliding windows and extracting features...\")\n",
    "    X_windowed, y_windowed = generate_sliding_windows(X_balanced, y_balanced)\n",
    "\n",
    "    print(\"Training Transformer model...\")\n",
    "    model, encoder = build_and_train_transformer(X_windowed, y_windowed)\n",
    "\n",
    "    print(\"Converting model to TFLite format...\")\n",
    "    convert_to_tflite(model)\n",
    "\n",
    "    print(\"Transformer training complete.\")\n",
    "    return model, encoder\n",
    "\n",
    "\n",
    "# üî• Execute the pipeline\n",
    "run_transformer_pipeline('.idea/df.csv')\n"
   ],
   "id": "475f95e5bacfa8cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Rebalancing classes using SMOTE...\n",
      "Generating sliding windows and extracting features...\n",
      "Training Transformer model...\n",
      "Epoch 1/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 38ms/step - accuracy: 0.4242 - loss: 1.3716 - val_accuracy: 0.6297 - val_loss: 0.9767\n",
      "Epoch 2/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 40ms/step - accuracy: 0.6210 - loss: 0.9831 - val_accuracy: 0.6571 - val_loss: 0.8681\n",
      "Epoch 3/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 39ms/step - accuracy: 0.6756 - loss: 0.8782 - val_accuracy: 0.6840 - val_loss: 0.8116\n",
      "Epoch 4/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 38ms/step - accuracy: 0.6864 - loss: 0.8397 - val_accuracy: 0.7208 - val_loss: 0.7593\n",
      "Epoch 5/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 38ms/step - accuracy: 0.7164 - loss: 0.7828 - val_accuracy: 0.7216 - val_loss: 0.7321\n",
      "Epoch 6/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 35ms/step - accuracy: 0.7188 - loss: 0.7653 - val_accuracy: 0.7380 - val_loss: 0.7102\n",
      "Epoch 7/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 36ms/step - accuracy: 0.7275 - loss: 0.7459 - val_accuracy: 0.7387 - val_loss: 0.6995\n",
      "Epoch 8/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 35ms/step - accuracy: 0.7400 - loss: 0.7135 - val_accuracy: 0.7577 - val_loss: 0.6875\n",
      "Epoch 9/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 38ms/step - accuracy: 0.7412 - loss: 0.7100 - val_accuracy: 0.7591 - val_loss: 0.6726\n",
      "Epoch 10/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 38ms/step - accuracy: 0.7378 - loss: 0.7116 - val_accuracy: 0.7533 - val_loss: 0.6632\n",
      "Epoch 11/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 35ms/step - accuracy: 0.7412 - loss: 0.6970 - val_accuracy: 0.7653 - val_loss: 0.6523\n",
      "Epoch 12/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 34ms/step - accuracy: 0.7570 - loss: 0.6721 - val_accuracy: 0.7770 - val_loss: 0.6376\n",
      "Epoch 13/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 33ms/step - accuracy: 0.7563 - loss: 0.6674 - val_accuracy: 0.7788 - val_loss: 0.6469\n",
      "Epoch 14/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 33ms/step - accuracy: 0.7576 - loss: 0.6490 - val_accuracy: 0.7806 - val_loss: 0.6200\n",
      "Epoch 15/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 40ms/step - accuracy: 0.7756 - loss: 0.6354 - val_accuracy: 0.7850 - val_loss: 0.6142\n",
      "Epoch 16/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 34ms/step - accuracy: 0.7610 - loss: 0.6477 - val_accuracy: 0.7926 - val_loss: 0.6051\n",
      "Epoch 17/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 39ms/step - accuracy: 0.7697 - loss: 0.6335 - val_accuracy: 0.7912 - val_loss: 0.6048\n",
      "Epoch 18/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 42ms/step - accuracy: 0.7763 - loss: 0.6222 - val_accuracy: 0.7945 - val_loss: 0.5935\n",
      "Epoch 19/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 38ms/step - accuracy: 0.7760 - loss: 0.6095 - val_accuracy: 0.8021 - val_loss: 0.5834\n",
      "Epoch 20/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 34ms/step - accuracy: 0.7619 - loss: 0.6416 - val_accuracy: 0.7959 - val_loss: 0.5932\n",
      "Epoch 21/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 37ms/step - accuracy: 0.7811 - loss: 0.5896 - val_accuracy: 0.7959 - val_loss: 0.5868\n",
      "Epoch 22/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 35ms/step - accuracy: 0.7843 - loss: 0.5900 - val_accuracy: 0.8028 - val_loss: 0.5641\n",
      "Epoch 23/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 35ms/step - accuracy: 0.7818 - loss: 0.6018 - val_accuracy: 0.8025 - val_loss: 0.5670\n",
      "Epoch 24/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 33ms/step - accuracy: 0.7779 - loss: 0.6014 - val_accuracy: 0.7966 - val_loss: 0.5849\n",
      "Epoch 25/25\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 31ms/step - accuracy: 0.7922 - loss: 0.5827 - val_accuracy: 0.8007 - val_loss: 0.5750\n",
      "\u001B[1m108/108\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Improved Transformer Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.70      0.75      0.72       680\n",
      "     running       0.94      0.82      0.88       631\n",
      "    standing       0.89      0.95      0.92       702\n",
      "    upstairs       0.79      0.77      0.78       712\n",
      "     walking       0.75      0.75      0.75       705\n",
      "\n",
      "    accuracy                           0.81      3430\n",
      "   macro avg       0.81      0.81      0.81      3430\n",
      "weighted avg       0.81      0.81      0.81      3430\n",
      "\n",
      "Transformer model saved to 'transformer_model.h5'\n",
      "Label encoder saved to 'transformer_label_encoder.pkl'\n",
      "Converting model to TFLite format...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\EGYPT\\AppData\\Local\\Temp\\tmpejojj_09\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\EGYPT\\AppData\\Local\\Temp\\tmpejojj_09\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\EGYPT\\AppData\\Local\\Temp\\tmpejojj_09'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 9), dtype=tf.float32, name='keras_tensor_12')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2156304991632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331958352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331958544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331959888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331960848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331960656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331959120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156331960272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156385694736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156385695120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156385693776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156385692048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156385694160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156385693008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite model saved to 'transformer_model.tflite'\n",
      "Transformer training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Functional name=functional_2, built=True>, LabelEncoder())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:24.610272Z",
     "start_time": "2025-05-11T12:19:01.214322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization,\n",
    "    MultiHeadAttention, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# --- Step 1: Load and preprocess data ---\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna()\n",
    "\n",
    "    X = df.drop('activity', axis=1)\n",
    "    y = df['activity']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "# --- Step 2: Apply SMOTE to balance classes ---\n",
    "def rebalance(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# --- Step 3: Generate sliding windows ---\n",
    "def generate_sliding_windows(X, y, window_size=50, step_size=25):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "\n",
    "    for start in range(0, len(X) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = X[start:end]\n",
    "        label = y[start:end]\n",
    "\n",
    "        # Use the most frequent label in the window\n",
    "        unique, counts = np.unique(label, return_counts=True)\n",
    "        dominant_label = unique[np.argmax(counts)]\n",
    "\n",
    "        X_windows.append(window)\n",
    "        y_windows.append(dominant_label)\n",
    "\n",
    "    return np.array(X_windows), np.array(y_windows)\n",
    "\n",
    "# --- Step 4: Positional Encoding ---\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(sequence_len, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, sequence_len, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(sequence_len)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "# --- Step 5: Transformer Encoder Block ---\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Multi-head self-attention\n",
    "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "    # Feed-forward\n",
    "    ffn = tf.keras.Sequential([\n",
    "        Dense(ff_dim, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(inputs.shape[-1])\n",
    "    ])\n",
    "    x2 = ffn(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + x2)\n",
    "    return x\n",
    "\n",
    "# --- Step 6: Build Transformer Model ---\n",
    "def build_transformer_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = PositionalEncoding(input_shape[0], input_shape[1])(inputs)\n",
    "\n",
    "    for _ in range(2):  # Stack multiple transformer blocks\n",
    "        x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Step 7: Train & Save Transformer Model ---\n",
    "def build_and_train_transformer(X, y, model_path='transformer_model.h5', encoder_path='transformer_label_encoder.pkl'):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = build_transformer_model(X.shape[1:], y_categorical.shape[1])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(\"\\n--- Transformer Model Evaluation ---\\n\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "    model.save(model_path)\n",
    "    joblib.dump(le, encoder_path)\n",
    "    print(f\"Transformer model saved to '{model_path}'\")\n",
    "    print(f\"Label encoder saved to '{encoder_path}'\")\n",
    "\n",
    "    return model, le\n",
    "\n",
    "# --- Step 8: Run Full Pipeline ---\n",
    "def run_transformer_pipeline(path):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_raw, y_raw = load_and_preprocess_data(path)\n",
    "\n",
    "    print(\"Rebalancing classes using SMOTE...\")\n",
    "    X_balanced, y_balanced = rebalance(X_raw, y_raw)\n",
    "\n",
    "    print(\"Generating sliding windows...\")\n",
    "    X_windowed, y_windowed = generate_sliding_windows(X_balanced, y_balanced)\n",
    "\n",
    "    print(\"Training Transformer model...\")\n",
    "    model, encoder = build_and_train_transformer(X_windowed, y_windowed)\n",
    "\n",
    "    print(\"Converting model to TFLite format...\")\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(\"transformer_model.tflite\", \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"TFLite model saved to 'transformer_model.tflite'\")\n",
    "\n",
    "    return model, encoder\n",
    "\n",
    "# üî• Run it!\n",
    "run_transformer_pipeline('.idea/df.csv')\n"
   ],
   "id": "7bc7390bc301468",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Rebalancing classes using SMOTE...\n",
      "Generating sliding windows...\n",
      "Training Transformer model...\n",
      "WARNING:tensorflow:From C:\\Users\\EGYPT\\anaconda3\\envs\\Mobile\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\EGYPT\\anaconda3\\envs\\Mobile\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 86ms/step - accuracy: 0.3119 - loss: 1.5384 - val_accuracy: 0.6013 - val_loss: 1.0471\n",
      "Epoch 2/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 89ms/step - accuracy: 0.5918 - loss: 1.0745 - val_accuracy: 0.7099 - val_loss: 0.7816\n",
      "Epoch 3/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 82ms/step - accuracy: 0.6744 - loss: 0.8926 - val_accuracy: 0.7609 - val_loss: 0.6904\n",
      "Epoch 4/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 80ms/step - accuracy: 0.7299 - loss: 0.7744 - val_accuracy: 0.7642 - val_loss: 0.6588\n",
      "Epoch 5/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 81ms/step - accuracy: 0.7441 - loss: 0.7358 - val_accuracy: 0.7773 - val_loss: 0.6345\n",
      "Epoch 6/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 76ms/step - accuracy: 0.7537 - loss: 0.7162 - val_accuracy: 0.7843 - val_loss: 0.6111\n",
      "Epoch 7/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 85ms/step - accuracy: 0.7751 - loss: 0.6734 - val_accuracy: 0.7937 - val_loss: 0.6301\n",
      "Epoch 8/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 83ms/step - accuracy: 0.7720 - loss: 0.6626 - val_accuracy: 0.7835 - val_loss: 0.6313\n",
      "Epoch 9/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 77ms/step - accuracy: 0.7814 - loss: 0.6333 - val_accuracy: 0.7628 - val_loss: 0.6566\n",
      "Epoch 10/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 77ms/step - accuracy: 0.7920 - loss: 0.6210 - val_accuracy: 0.7945 - val_loss: 0.6021\n",
      "Epoch 11/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 77ms/step - accuracy: 0.7972 - loss: 0.5944 - val_accuracy: 0.8003 - val_loss: 0.5824\n",
      "Epoch 12/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 76ms/step - accuracy: 0.7998 - loss: 0.5920 - val_accuracy: 0.7664 - val_loss: 0.6758\n",
      "Epoch 13/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 81ms/step - accuracy: 0.7973 - loss: 0.5867 - val_accuracy: 0.7952 - val_loss: 0.5909\n",
      "Epoch 14/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 77ms/step - accuracy: 0.8074 - loss: 0.5633 - val_accuracy: 0.7966 - val_loss: 0.5976\n",
      "Epoch 15/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 82ms/step - accuracy: 0.8080 - loss: 0.5635 - val_accuracy: 0.8105 - val_loss: 0.5853\n",
      "Epoch 16/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 80ms/step - accuracy: 0.8183 - loss: 0.5235 - val_accuracy: 0.8196 - val_loss: 0.5545\n",
      "Epoch 17/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 83ms/step - accuracy: 0.8165 - loss: 0.5457 - val_accuracy: 0.8225 - val_loss: 0.5274\n",
      "Epoch 18/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 72ms/step - accuracy: 0.8202 - loss: 0.5192 - val_accuracy: 0.8043 - val_loss: 0.5919\n",
      "Epoch 19/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 72ms/step - accuracy: 0.8277 - loss: 0.5125 - val_accuracy: 0.8174 - val_loss: 0.5385\n",
      "Epoch 20/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 75ms/step - accuracy: 0.8328 - loss: 0.5022 - val_accuracy: 0.7992 - val_loss: 0.6361\n",
      "Epoch 21/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 73ms/step - accuracy: 0.8308 - loss: 0.5048 - val_accuracy: 0.8174 - val_loss: 0.5725\n",
      "Epoch 22/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 75ms/step - accuracy: 0.8378 - loss: 0.4820 - val_accuracy: 0.8345 - val_loss: 0.5093\n",
      "Epoch 23/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 73ms/step - accuracy: 0.8295 - loss: 0.4925 - val_accuracy: 0.8116 - val_loss: 0.5666\n",
      "Epoch 24/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 73ms/step - accuracy: 0.8363 - loss: 0.4978 - val_accuracy: 0.8207 - val_loss: 0.5339\n",
      "Epoch 25/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 77ms/step - accuracy: 0.8438 - loss: 0.4614 - val_accuracy: 0.8225 - val_loss: 0.5523\n",
      "Epoch 26/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 78ms/step - accuracy: 0.8510 - loss: 0.4552 - val_accuracy: 0.8189 - val_loss: 0.5983\n",
      "Epoch 27/50\n",
      "\u001B[1m172/172\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 75ms/step - accuracy: 0.8352 - loss: 0.4751 - val_accuracy: 0.8214 - val_loss: 0.5641\n",
      "\u001B[1m108/108\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transformer Model Evaluation ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  downstairs       0.68      0.88      0.77       680\n",
      "     running       0.95      0.87      0.91       631\n",
      "    standing       0.93      0.94      0.93       702\n",
      "    upstairs       0.81      0.75      0.78       712\n",
      "     walking       0.86      0.76      0.81       705\n",
      "\n",
      "    accuracy                           0.84      3430\n",
      "   macro avg       0.85      0.84      0.84      3430\n",
      "weighted avg       0.85      0.84      0.84      3430\n",
      "\n",
      "Transformer model saved to 'transformer_model.h5'\n",
      "Label encoder saved to 'transformer_label_encoder.pkl'\n",
      "Converting model to TFLite format...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\EGYPT\\AppData\\Local\\Temp\\tmpf8a_mjr4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\EGYPT\\AppData\\Local\\Temp\\tmpf8a_mjr4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\EGYPT\\AppData\\Local\\Temp\\tmpf8a_mjr4'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 9), dtype=tf.float32, name='keras_tensor_20')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2156521755792: TensorSpec(shape=(1, 50, 9), dtype=tf.float32, name=None)\n",
      "  2156304990288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521752144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521750608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521760976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521752336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521761360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521760784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521760016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521758864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521761552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521759440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521757328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521757904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521756752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521758288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521756944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521761936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521762512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521752912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521763280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521762704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521764048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521763472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521764816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521765200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521758480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521765008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521766544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521764624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156521755600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156388908304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156388908496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156388908112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156388893136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156388893712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2156388893904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite model saved to 'transformer_model.tflite'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Functional name=functional_5, built=True>, LabelEncoder())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7d1f468e37619841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89c785a97795418d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:23:57.166649Z",
     "start_time": "2025-05-14T16:13:43.570135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 1. Load HAR dataset\n",
    "def load_har_dataset(data_dir):\n",
    "    # Features\n",
    "    X_train = pd.read_csv(os.path.join(data_dir, \"train\", \"X_train.txt\"), delim_whitespace=True, header=None)\n",
    "    X_test = pd.read_csv(os.path.join(data_dir, \"test\", \"X_test.txt\"), delim_whitespace=True, header=None)\n",
    "\n",
    "    # Labels\n",
    "    y_train = pd.read_csv(os.path.join(data_dir, \"train\", \"y_train.txt\"), delim_whitespace=True, header=None)[0]\n",
    "    y_test = pd.read_csv(os.path.join(data_dir, \"test\", \"y_test.txt\"), delim_whitespace=True, header=None)[0]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 2. Train AdaBoost model\n",
    "def train_adaboost(X_train, y_train):\n",
    "    print(\"üéØ Training AdaBoost model...\")\n",
    "    model = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=5),\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# 3. Evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\nüìà Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# 4. Save model\n",
    "def save_model(model, path=\"har_adaboost_model.pkl\"):\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"üíæ Model saved to: {path}\")\n",
    "\n",
    "# 5. Main\n",
    "def main():\n",
    "    data_dir = \"./UCI HAR Dataset\"\n",
    "    X_train, X_test, y_train, y_test = load_har_dataset(data_dir)\n",
    "\n",
    "    model = train_adaboost(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    save_model(model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "1ec5aa5ac9eff75b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_13540\\4117802177.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_train = pd.read_csv(os.path.join(data_dir, \"train\", \"X_train.txt\"), delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_13540\\4117802177.py:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_test = pd.read_csv(os.path.join(data_dir, \"test\", \"X_test.txt\"), delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_13540\\4117802177.py:17: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_train = pd.read_csv(os.path.join(data_dir, \"train\", \"y_train.txt\"), delim_whitespace=True, header=None)[0]\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_13540\\4117802177.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_test = pd.read_csv(os.path.join(data_dir, \"test\", \"y_test.txt\"), delim_whitespace=True, header=None)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training AdaBoost model...\n",
      "\n",
      "üìà Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.96      0.95       496\n",
      "           2       0.90      0.94      0.92       471\n",
      "           3       0.98      0.92      0.95       420\n",
      "           4       0.95      0.87      0.91       491\n",
      "           5       0.89      0.96      0.92       532\n",
      "           6       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.94      2947\n",
      "   macro avg       0.94      0.94      0.94      2947\n",
      "weighted avg       0.94      0.94      0.94      2947\n",
      "\n",
      "‚úÖ Accuracy: 0.9427\n",
      "üíæ Model saved to: har_adaboost_model.pkl\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82fc11f670935691"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96c36e043ef8e92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:34:49.170790Z",
     "start_time": "2025-05-15T17:34:00.163820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import joblib\n",
    "\n",
    "# Paths (Save directly in the current directory)\n",
    "PKL_PATH = \"har_rf_model.pkl\"\n",
    "ONNX_PATH = \"har_rf_model.onnx\"\n",
    "DATASET_DIR = \"C:\\\\Users\\\\EGYPT\\\\Downloads\\\\src\\\\Mobile\\\\UCI HAR Dataset\"\n",
    "\n",
    "def load_signal(sensor, axis, split=\"train\"):\n",
    "    path = f\"{DATASET_DIR}/{split}/Inertial Signals/{sensor}_{axis}_{split}.txt\"\n",
    "    return pd.read_csv(path, delim_whitespace=True, header=None)\n",
    "\n",
    "def extract_features_and_labels(split=\"train\"):\n",
    "    sensors = [\"body_acc\", \"body_gyro\", \"total_acc\"]\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "    signals = [load_signal(s, a, split) for s in sensors for a in axes]\n",
    "\n",
    "    features = []\n",
    "    for i in range(signals[0].shape[0]):\n",
    "        f = []\n",
    "        for sig in signals:\n",
    "            row = sig.iloc[i]\n",
    "            f.extend([row.mean(), row.std(), row.min(), row.max()])\n",
    "        features.append(f)\n",
    "\n",
    "    labels_path = f\"{DATASET_DIR}/{split}/y_{split}.txt\"\n",
    "    labels = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0]\n",
    "    labels = labels - 1  # Shift to 0‚Äì5\n",
    "    return np.array(features), labels\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = extract_features_and_labels(\"train\")\n",
    "X_test, y_test = extract_features_and_labels(\"test\")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"üìä Random Forest Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Save model in current directory\n",
    "joblib.dump(model, PKL_PATH)\n",
    "\n",
    "# Export to ONNX in current directory\n",
    "initial_type = [('float_input', FloatTensorType([None, 36]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(ONNX_PATH, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úÖ Random Forest model saved to: {PKL_PATH}\")\n",
    "print(f\"‚úÖ Random Forest ONNX saved to: {ONNX_PATH}\")\n"
   ],
   "id": "8db4c5a65423e766",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:34: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0]\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\965019241.py:34: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70       496\n",
      "           1       0.70      0.72      0.71       471\n",
      "           2       0.89      0.78      0.83       420\n",
      "           3       0.89      0.81      0.85       491\n",
      "           4       0.84      0.91      0.87       532\n",
      "           5       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.83      2947\n",
      "   macro avg       0.83      0.82      0.83      2947\n",
      "weighted avg       0.83      0.83      0.83      2947\n",
      "\n",
      "‚úÖ Accuracy: 0.8297\n",
      "‚úÖ Random Forest model saved to: har_rf_model.pkl\n",
      "‚úÖ Random Forest ONNX saved to: har_rf_model.onnx\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:49:11.628989Z",
     "start_time": "2025-05-15T17:47:58.866855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import joblib\n",
    "\n",
    "# Paths\n",
    "PKL_PATH = \"har_adaboost_model.pkl\"\n",
    "ONNX_PATH = \"har_adaboost_model.onnx\"\n",
    "DATASET_DIR = \"C:\\\\Users\\\\EGYPT\\\\Downloads\\\\src\\\\Mobile\\\\UCI HAR Dataset\"\n",
    "\n",
    "def load_signal(sensor, axis, split=\"train\"):\n",
    "    path = f\"{DATASET_DIR}/{split}/Inertial Signals/{sensor}_{axis}_{split}.txt\"\n",
    "    return pd.read_csv(path, delim_whitespace=True, header=None)\n",
    "\n",
    "def extract_features_and_labels(split=\"train\"):\n",
    "    sensors = [\"body_acc\", \"body_gyro\", \"total_acc\"]\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "    signals = [load_signal(s, a, split) for s in sensors for a in axes]\n",
    "\n",
    "    features = []\n",
    "    for i in range(signals[0].shape[0]):\n",
    "        f = []\n",
    "        for sig in signals:\n",
    "            row = sig.iloc[i]\n",
    "            f.extend([\n",
    "                row.mean(),\n",
    "                row.std(),\n",
    "                row.min(),\n",
    "                row.max(),\n",
    "                np.sum(row**2)/len(row),        # Energy\n",
    "                np.sum(np.abs(row))/len(row)    # SMA\n",
    "            ])\n",
    "        features.append(f)\n",
    "\n",
    "    labels_path = f\"{DATASET_DIR}/{split}/y_{split}.txt\"\n",
    "    labels = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0]\n",
    "    labels = labels - 1  # classes 0‚Äì5\n",
    "    return np.array(features), labels\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = extract_features_and_labels(\"train\")\n",
    "X_test, y_test = extract_features_and_labels(\"test\")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train AdaBoost\n",
    "model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"üìä AdaBoost Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"üéØ F1 Macro Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, PKL_PATH)\n",
    "\n",
    "# Export to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(ONNX_PATH, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úÖ AdaBoost model saved to: {PKL_PATH}\")\n",
    "print(f\"‚úÖ AdaBoost ONNX saved to: {ONNX_PATH}\")\n"
   ],
   "id": "588c714080a4cbcc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:42: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0]\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2760123075.py:42: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0]\n",
      "C:\\Users\\EGYPT\\anaconda3\\envs\\Mobile\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\EGYPT\\anaconda3\\envs\\Mobile\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\EGYPT\\anaconda3\\envs\\Mobile\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä AdaBoost Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       496\n",
      "           1       0.44      0.86      0.58       471\n",
      "           2       0.75      0.82      0.78       420\n",
      "           3       0.75      0.01      0.01       491\n",
      "           4       0.52      1.00      0.68       532\n",
      "           5       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.62      2947\n",
      "   macro avg       0.58      0.61      0.51      2947\n",
      "weighted avg       0.58      0.62      0.51      2947\n",
      "\n",
      "‚úÖ Accuracy: 0.6179\n",
      "üéØ F1 Macro Score: 0.5099\n",
      "‚úÖ AdaBoost model saved to: har_adaboost_model.pkl\n",
      "‚úÖ AdaBoost ONNX saved to: har_adaboost_model.onnx\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:26:18.257892Z",
     "start_time": "2025-05-15T18:18:47.122242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D, Add\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "\n",
    "DATASET_DIR = \"C:\\\\Users\\\\EGYPT\\\\Downloads\\\\src\\\\Mobile\\\\UCI HAR Dataset\"\n",
    "\n",
    "def load_signal(sensor, axis, split=\"train\"):\n",
    "    path = f\"{DATASET_DIR}/{split}/Inertial Signals/{sensor}_{axis}_{split}.txt\"\n",
    "    return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
    "\n",
    "def load_data(split=\"train\"):\n",
    "    sensors = [\"body_acc\", \"body_gyro\", \"total_acc\"]\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "    signals = [load_signal(s, a, split) for s in sensors for a in axes]\n",
    "    X = np.stack(signals, axis=-1)  # Shape: (samples, time_steps, features)\n",
    "\n",
    "    labels_path = f\"{DATASET_DIR}/{split}/y_{split}.txt\"\n",
    "    y = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0] - 1\n",
    "    return X, to_categorical(y, num_classes=6)\n",
    "\n",
    "# Load and scale data\n",
    "X_train, y_train = load_data(\"train\")\n",
    "X_test, y_test = load_data(\"test\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "nsamples, ntimesteps, nfeatures = X_train.shape\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, nfeatures)).reshape(nsamples, ntimesteps, nfeatures)\n",
    "X_test = scaler.transform(X_test.reshape(-1, nfeatures)).reshape(X_test.shape[0], ntimesteps, nfeatures)\n",
    "\n",
    "# Transformer block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = Add()([x, inputs])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return Add()([x, res])\n",
    "\n",
    "# Build model\n",
    "def build_transformer_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128)\n",
    "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model = build_transformer_model(input_shape=(X_train.shape[1], X_train.shape[2]), num_classes=6)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(1e-4), metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=15, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"üìä Transformer Classifier Report:\\n\", classification_report(y_true, y_pred_labels))\n",
    "print(f\"‚úÖ Accuracy: {accuracy_score(y_true, y_pred_labels):.4f}\")\n",
    "\n",
    "model.save(\"har_transformer_model.h5\")\n",
    "print(\"‚úÖ Transformer Keras model saved to har_transformer_model.h5\")\n",
    "\n",
    "\n",
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "spec = (tf.TensorSpec(model.input.shape, tf.float32, name=\"input\"),)\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "\n",
    "onnx.save(onnx_model, \"har_transformer_model.onnx\")\n",
    "print(\"‚úÖ Transformer ONNX model saved to har_transformer_model.onnx\")\n",
    "\n"
   ],
   "id": "43a3772d3221b545",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0] - 1\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\572518336.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y = pd.read_csv(labels_path, delim_whitespace=True, header=None)[0] - 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_3       ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mInputLayer\u001B[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ         \u001B[38;5;34m18\u001B[0m ‚îÇ input_layer_3[\u001B[38;5;34m0\u001B[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mLayerNormalizatio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attenti‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ      \u001B[38;5;34m9,993\u001B[0m ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mMultiHeadAttentio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_9 (\u001B[38;5;33mDropout\u001B[0m) ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ multi_head_atten‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_4 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dropout_9[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ input_layer_3[\u001B[38;5;34m0\u001B[0m]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ         \u001B[38;5;34m18\u001B[0m ‚îÇ add_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mLayerNormalizatio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_8 (\u001B[38;5;33mDense\u001B[0m)     ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ      \u001B[38;5;34m1,280\u001B[0m ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_10          ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dense_8[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_9 (\u001B[38;5;33mDense\u001B[0m)     ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ      \u001B[38;5;34m1,161\u001B[0m ‚îÇ dropout_10[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_5 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dense_9[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],    ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ         \u001B[38;5;34m18\u001B[0m ‚îÇ add_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mLayerNormalizatio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attenti‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ      \u001B[38;5;34m9,993\u001B[0m ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mMultiHeadAttentio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_12          ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ multi_head_atten‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_6 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dropout_12[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m], ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ         \u001B[38;5;34m18\u001B[0m ‚îÇ add_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mLayerNormalizatio‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_10 (\u001B[38;5;33mDense\u001B[0m)    ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ      \u001B[38;5;34m1,280\u001B[0m ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_13          ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m128\u001B[0m)  ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dense_10[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_11 (\u001B[38;5;33mDense\u001B[0m)    ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ      \u001B[38;5;34m1,161\u001B[0m ‚îÇ dropout_13[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_7 (\u001B[38;5;33mAdd\u001B[0m)         ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m, \u001B[38;5;34m9\u001B[0m)    ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dense_11[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],   ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_poo‚Ä¶ ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m)         ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ add_7[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mGlobalAveragePool‚Ä¶\u001B[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_14          ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m)         ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ global_average_p‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_12 (\u001B[38;5;33mDense\u001B[0m)    ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)        ‚îÇ        \u001B[38;5;34m640\u001B[0m ‚îÇ dropout_14[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_15          ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)        ‚îÇ          \u001B[38;5;34m0\u001B[0m ‚îÇ dense_12[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    ‚îÇ\n",
       "‚îÇ (\u001B[38;5;33mDropout\u001B[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_13 (\u001B[38;5;33mDense\u001B[0m)    ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)         ‚îÇ        \u001B[38;5;34m390\u001B[0m ‚îÇ dropout_15[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]  ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_3       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> ‚îÇ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attenti‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,993</span> ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ multi_head_atten‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> ‚îÇ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_10          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> ‚îÇ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> ‚îÇ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attenti‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,993</span> ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_12          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ multi_head_atten‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> ‚îÇ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> ‚îÇ layer_normalizat‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_13          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> ‚îÇ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   ‚îÇ\n",
       "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_poo‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_14          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ global_average_p‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> ‚îÇ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_15          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> ‚îÇ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m25,970\u001B[0m (101.45 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,970</span> (101.45 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m25,970\u001B[0m (101.45 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,970</span> (101.45 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 261ms/step - accuracy: 0.1647 - loss: 1.8407 - val_accuracy: 0.4650 - val_loss: 1.4793\n",
      "Epoch 2/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 254ms/step - accuracy: 0.4308 - loss: 1.4641 - val_accuracy: 0.5969 - val_loss: 1.2207\n",
      "Epoch 3/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 249ms/step - accuracy: 0.5100 - loss: 1.2780 - val_accuracy: 0.6322 - val_loss: 1.0689\n",
      "Epoch 4/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 276ms/step - accuracy: 0.5917 - loss: 1.1070 - val_accuracy: 0.6615 - val_loss: 0.9781\n",
      "Epoch 5/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 277ms/step - accuracy: 0.6215 - loss: 0.9946 - val_accuracy: 0.6635 - val_loss: 0.9082\n",
      "Epoch 6/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 307ms/step - accuracy: 0.6629 - loss: 0.9020 - val_accuracy: 0.6791 - val_loss: 0.8511\n",
      "Epoch 7/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 292ms/step - accuracy: 0.6625 - loss: 0.8810 - val_accuracy: 0.6982 - val_loss: 0.8019\n",
      "Epoch 8/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 276ms/step - accuracy: 0.6840 - loss: 0.8079 - val_accuracy: 0.7587 - val_loss: 0.7501\n",
      "Epoch 9/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 274ms/step - accuracy: 0.7007 - loss: 0.7609 - val_accuracy: 0.7791 - val_loss: 0.7086\n",
      "Epoch 10/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 272ms/step - accuracy: 0.7232 - loss: 0.7055 - val_accuracy: 0.8266 - val_loss: 0.6777\n",
      "Epoch 11/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 291ms/step - accuracy: 0.7262 - loss: 0.6909 - val_accuracy: 0.8239 - val_loss: 0.6419\n",
      "Epoch 12/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 304ms/step - accuracy: 0.7335 - loss: 0.6664 - val_accuracy: 0.8423 - val_loss: 0.6127\n",
      "Epoch 13/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 286ms/step - accuracy: 0.7575 - loss: 0.6267 - val_accuracy: 0.8647 - val_loss: 0.5749\n",
      "Epoch 14/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 303ms/step - accuracy: 0.7810 - loss: 0.5648 - val_accuracy: 0.8566 - val_loss: 0.5461\n",
      "Epoch 15/15\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 321ms/step - accuracy: 0.8025 - loss: 0.5345 - val_accuracy: 0.8817 - val_loss: 0.5087\n",
      "\u001B[1m93/93\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 70ms/step\n",
      "üìä Transformer Classifier Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.86      0.76       496\n",
      "           1       0.87      0.68      0.76       471\n",
      "           2       0.84      0.80      0.82       420\n",
      "           3       0.83      0.78      0.80       491\n",
      "           4       0.79      0.85      0.82       532\n",
      "           5       0.99      0.96      0.98       537\n",
      "\n",
      "    accuracy                           0.83      2947\n",
      "   macro avg       0.83      0.82      0.82      2947\n",
      "weighted avg       0.84      0.83      0.83      2947\n",
      "\n",
      "‚úÖ Accuracy: 0.8266\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:26:18.687048Z",
     "start_time": "2025-05-15T18:26:18.435436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save(\"har_transformer_model.h5\")\n",
    "print(\"‚úÖ Transformer Keras model saved to har_transformer_model.h5\")\n"
   ],
   "id": "6c79d3f78fe1542c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformer Keras model saved to har_transformer_model.h5\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:26:29.327273Z",
     "start_time": "2025-05-15T18:26:18.837078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "spec = (tf.TensorSpec(model.input.shape, tf.float32, name=\"input\"),)\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "\n",
    "onnx.save(onnx_model, \"har_transformer_model.onnx\")\n",
    "print(\"‚úÖ Transformer ONNX model saved to har_transformer_model.onnx\")\n"
   ],
   "id": "6de8e2945680ce5a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tf2onnx.tfonnx:rewriter <function rewrite_constant_fold at 0x00000276D3BA74C0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformer ONNX model saved to har_transformer_model.onnx\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:41:47.461221Z",
     "start_time": "2025-05-15T18:38:47.189491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "# Paths\n",
    "DATASET_DIR = \"C:\\\\Users\\\\EGYPT\\\\Downloads\\\\src\\\\Mobile\\\\UCI HAR Dataset\"\n",
    "WINDOW_SIZE = 50\n",
    "STEP = 25\n",
    "N_FEATURES = 9\n",
    "PKL_PATH = \"transformer_model.pkl\"\n",
    "ONNX_PATH = \"transformer_model.onnx\"\n",
    "\n",
    "# Load raw signal data\n",
    "def load_signal(sensor, axis, split):\n",
    "    file_path = f\"{DATASET_DIR}/{split}/Inertial Signals/{sensor}_{axis}_{split}.txt\"\n",
    "    return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
    "\n",
    "# Create windowed dataset\n",
    "def create_windowed_data(split=\"train\"):\n",
    "    sensors = [\"body_acc\", \"body_gyro\", \"total_acc\"]\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "    signals = [load_signal(s, a, split) for s in sensors for a in axes]\n",
    "    signals = np.stack(signals, axis=-1)  # shape: (samples, 128, 9)\n",
    "\n",
    "    X, y = [], []\n",
    "    labels_path = f\"{DATASET_DIR}/{split}/y_{split}.txt\"\n",
    "    labels = pd.read_csv(labels_path, delim_whitespace=True, header=None).values.flatten() - 1\n",
    "\n",
    "    for i in range(signals.shape[0]):\n",
    "        sequence = signals[i]\n",
    "        label = labels[i]\n",
    "        for start in range(0, sequence.shape[0] - WINDOW_SIZE + 1, STEP):\n",
    "            window = sequence[start:start + WINDOW_SIZE]\n",
    "            X.append(window)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and scale data\n",
    "X_train, y_train = create_windowed_data(\"train\")\n",
    "X_test, y_test = create_windowed_data(\"test\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, N_FEATURES)).reshape(-1, WINDOW_SIZE, N_FEATURES)\n",
    "X_test = scaler.transform(X_test.reshape(-1, N_FEATURES)).reshape(-1, WINDOW_SIZE, N_FEATURES)\n",
    "joblib.dump(scaler, \"transformer_scaler.pkl\")\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "joblib.dump(encoder, \"transformer_label_encoder.pkl\")\n",
    "\n",
    "# Build Transformer model\n",
    "def build_transformer_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LayerNormalization()(inputs)\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model = build_transformer_model((WINDOW_SIZE, N_FEATURES), len(np.unique(y_train)))\n",
    "model.compile(optimizer=Adam(1e-4), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(\"üìä Transformer Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Save model and encoder\n",
    "model.save(\"transformer_model.h5\")\n",
    "joblib.dump(model, PKL_PATH)\n",
    "\n",
    "# Convert to ONNX\n",
    "spec = (tf.TensorSpec((None, WINDOW_SIZE, N_FEATURES), tf.float32, name=\"input\"),)\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=ONNX_PATH)\n",
    "print(f\"‚úÖ Saved Transformer model to {PKL_PATH} and {ONNX_PATH}\")\n"
   ],
   "id": "1dcb530b44f4b1d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv(labels_path, delim_whitespace=True, header=None).values.flatten() - 1\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  return pd.read_csv(file_path, delim_whitespace=True, header=None).values\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_7872\\2876098743.py:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels = pd.read_csv(labels_path, delim_whitespace=True, header=None).values.flatten() - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 35ms/step - accuracy: 0.5649 - loss: 1.4282 - val_accuracy: 0.7428 - val_loss: 0.8797\n",
      "Epoch 2/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 38ms/step - accuracy: 0.7833 - loss: 0.8133 - val_accuracy: 0.8035 - val_loss: 0.6118\n",
      "Epoch 3/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 37ms/step - accuracy: 0.8568 - loss: 0.5497 - val_accuracy: 0.8674 - val_loss: 0.4530\n",
      "Epoch 4/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 35ms/step - accuracy: 0.8959 - loss: 0.3820 - val_accuracy: 0.8861 - val_loss: 0.3835\n",
      "Epoch 5/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 35ms/step - accuracy: 0.9105 - loss: 0.2957 - val_accuracy: 0.8944 - val_loss: 0.3370\n",
      "Epoch 6/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 35ms/step - accuracy: 0.9167 - loss: 0.2539 - val_accuracy: 0.9043 - val_loss: 0.3384\n",
      "Epoch 7/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 34ms/step - accuracy: 0.9232 - loss: 0.2264 - val_accuracy: 0.9029 - val_loss: 0.3339\n",
      "Epoch 8/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 37ms/step - accuracy: 0.9240 - loss: 0.2151 - val_accuracy: 0.9050 - val_loss: 0.3375\n",
      "Epoch 9/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 37ms/step - accuracy: 0.9288 - loss: 0.1983 - val_accuracy: 0.9087 - val_loss: 0.3360\n",
      "Epoch 10/20\n",
      "\u001B[1m368/368\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 38ms/step - accuracy: 0.9280 - loss: 0.1925 - val_accuracy: 0.9024 - val_loss: 0.3465\n",
      "\u001B[1m369/369\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Transformer Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      1984\n",
      "           1       0.85      0.81      0.83      1884\n",
      "           2       0.77      0.87      0.81      1680\n",
      "           3       0.84      0.75      0.79      1964\n",
      "           4       0.80      0.86      0.83      2128\n",
      "           5       1.00      0.97      0.98      2148\n",
      "\n",
      "    accuracy                           0.85     11788\n",
      "   macro avg       0.85      0.85      0.85     11788\n",
      "weighted avg       0.85      0.85      0.85     11788\n",
      "\n",
      "‚úÖ Accuracy: 0.8497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tf2onnx.tfonnx:rewriter <function rewrite_constant_fold at 0x00000276D3BA74C0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved Transformer model to transformer_model.pkl and transformer_model.onnx\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:46:49.058326Z",
     "start_time": "2025-05-15T18:46:43.020440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "spec = (tf.TensorSpec(model.input.shape, tf.float32, name=\"input\"),)\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "\n",
    "onnx.save(onnx_model, \"har_transformer_model.onnx\")\n",
    "print(\"‚úÖ Transformer ONNX model saved to har_transformer_model.onnx\")\n"
   ],
   "id": "8f17438710606723",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tf2onnx.tfonnx:rewriter <function rewrite_constant_fold at 0x00000276D3BA74C0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformer ONNX model saved to har_transformer_model.onnx\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:18:11.873325Z",
     "start_time": "2025-05-15T20:07:49.194787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import joblib\n",
    "from scipy.stats import skew, kurtosis\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Paths\n",
    "DATASET_DIR = \"C:\\\\Users\\\\EGYPT\\\\Downloads\\\\src\\\\Mobile\\\\UCI HAR Dataset\"\n",
    "PKL_PATH = \"../Models/har_rf_model_v2.pkl\"\n",
    "ONNX_PATH = \"../Models/har_rf_model_v2.onnx\"\n",
    "\n",
    "def load_signal(sensor, axis, split=\"train\"):\n",
    "    path = f\"{DATASET_DIR}/{split}/Inertial Signals/{sensor}_{axis}_{split}.txt\"\n",
    "    return pd.read_csv(path, sep='\\s+', header=None)\n",
    "\n",
    "def extract_features_and_labels(split=\"train\"):\n",
    "    sensors = [\"body_acc\", \"body_gyro\", \"total_acc\"]\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "    signals = [load_signal(s, a, split) for s in sensors for a in axes]\n",
    "\n",
    "    features = []\n",
    "    for i in range(signals[0].shape[0]):\n",
    "        f = []\n",
    "        for sig in signals:\n",
    "            row = sig.iloc[i]\n",
    "            # Add 9 stats per signal: mean, std, min, max, median, energy, range, skewness, kurtosis\n",
    "            vals = row.values\n",
    "            f.extend([\n",
    "                np.mean(vals),\n",
    "                np.std(vals),\n",
    "                np.min(vals),\n",
    "                np.max(vals),\n",
    "                np.median(vals),\n",
    "                np.sum(vals ** 2) / len(vals),  # energy\n",
    "                np.max(vals) - np.min(vals),    # range\n",
    "                skew(vals),\n",
    "                kurtosis(vals)\n",
    "            ])\n",
    "        features.append(f)\n",
    "\n",
    "    labels_path = f\"{DATASET_DIR}/{split}/y_{split}.txt\"\n",
    "    labels = pd.read_csv(labels_path, sep='\\s+', header=None)[0]\n",
    "    labels = labels - 1  # Shift to 0-5\n",
    "    return np.array(features), labels\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = extract_features_and_labels(\"train\")\n",
    "X_test, y_test = extract_features_and_labels(\"test\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [20, 30, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"üìä AdaBoost Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"üéØ F1 Macro Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, PKL_PATH)\n",
    "\n",
    "# Export to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(ONNX_PATH, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úÖ AdaBoost model saved to: {PKL_PATH}\")\n",
    "print(f\"‚úÖ AdaBoost ONNX saved to: {ONNX_PATH}\")"
   ],
   "id": "c81ecdb8349a5084",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_4972\\1175629077.py:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return pd.read_csv(path, sep='\\s+', header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_4972\\1175629077.py:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  labels = pd.read_csv(labels_path, sep='\\s+', header=None)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä AdaBoost Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.68       496\n",
      "           1       0.70      0.75      0.72       471\n",
      "           2       0.91      0.85      0.88       420\n",
      "           3       0.88      0.84      0.86       491\n",
      "           4       0.86      0.89      0.88       532\n",
      "           5       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.84      2947\n",
      "   macro avg       0.84      0.84      0.84      2947\n",
      "weighted avg       0.84      0.84      0.84      2947\n",
      "\n",
      "‚úÖ Accuracy: 0.8392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_4972\\1175629077.py:21: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return pd.read_csv(path, sep='\\s+', header=None)\n",
      "C:\\Users\\EGYPT\\AppData\\Local\\Temp\\ipykernel_4972\\1175629077.py:49: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  labels = pd.read_csv(labels_path, sep='\\s+', header=None)[0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 85\u001B[39m\n\u001B[32m     83\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33müìä AdaBoost Report:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, classification_report(y_test, y_pred))\n\u001B[32m     84\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m‚úÖ Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy_score(y_test,\u001B[38;5;250m \u001B[39my_pred)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33müéØ F1 Macro Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mf1_score\u001B[49m(y_test,\u001B[38;5;250m \u001B[39my_pred,\u001B[38;5;250m \u001B[39maverage=\u001B[33m'\u001B[39m\u001B[33mmacro\u001B[39m\u001B[33m'\u001B[39m)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     87\u001B[39m \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[32m     88\u001B[39m joblib.dump(model, PKL_PATH)\n",
      "\u001B[31mNameError\u001B[39m: name 'f1_score' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca7176eaf1e2bb25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5580c9afc075e3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0885bc37c24f85f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9307b31205b5b125"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "DATASET_DIR = \"/src/Mobile/UCI HAR Dataset\"\n",
    "PKL_PATH = \"../Models/har_rf_model_op1_smote.pkl\"\n",
    "ONNX_PATH = \"../Models/har_rf_model_op1_smote.onnx\"\n",
    "\n",
    "def load_signal(sensor, axis, split=\"train\"):\n",
    "    path = f\"{DATASET_DIR}/{split}/Inertial Signals/{sensor}_{axis}_{split}.txt\"\n",
    "    return pd.read_csv(path, sep='\\s+', header=None)\n",
    "\n",
    "def extract_features_and_labels(split=\"train\"):\n",
    "    sensors = [\"body_acc\", \"body_gyro\", \"total_acc\"]\n",
    "    axes = [\"x\", \"y\", \"z\"]\n",
    "    signals = [load_signal(s, a, split) for s in sensors for a in axes]\n",
    "\n",
    "    features = []\n",
    "    for i in range(signals[0].shape[0]):\n",
    "        f = []\n",
    "        for sig in signals:\n",
    "            row = sig.iloc[i]\n",
    "            f.extend([row.mean(), row.std(), row.min(), row.max()])\n",
    "        features.append(f)\n",
    "\n",
    "    labels_path = f\"{DATASET_DIR}/{split}/y_{split}.txt\"\n",
    "    labels = pd.read_csv(labels_path, sep='\\s+', header=None)[0]\n",
    "    labels = labels - 1  # Convert to 0‚Äì5\n",
    "    return np.array(features), labels\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = extract_features_and_labels(\"train\")\n",
    "X_test, y_test = extract_features_and_labels(\"test\")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE\n",
    "print(\"üîÅ Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "print(f\"‚úÖ Resampled shape: {X_train_bal.shape}, {np.bincount(y_train_bal)}\")\n",
    "\n",
    "# Optimized Random Forest\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=30,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    class_weight=None,  # No need for class_weight with SMOTE\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"üìä Random Forest + SMOTE Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"üîç Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature Importance Plot (Optional)\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), indices, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model and scaler\n",
    "os.makedirs(os.path.dirname(PKL_PATH), exist_ok=True)\n",
    "joblib.dump(scaler, PKL_PATH.replace(\".pkl\", \"_scaler.pkl\"))\n",
    "joblib.dump(model, PKL_PATH)\n",
    "\n",
    "# Export to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, 36]))]\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(ONNX_PATH, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úÖ RF + SMOTE ONNX saved to: {ONNX_PATH}\")"
   ],
   "id": "84018f90fa53c1f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13a52a0a569d8576"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
